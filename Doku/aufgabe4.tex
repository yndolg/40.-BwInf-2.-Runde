\documentclass[a4paper,10pt,ngerman]{scrartcl}

\input{preamble.tex}

\begin{document}

\maketitle
\tableofcontents

\section{Lösungsidee}
\subsection{Modellierung}
Gegeben sind $n$ Karten, die aus jeweils $m$ Bits bestehen. Das exklusive Oder von $k$ dieser Karten soll eine weitere Karte ergben.
Da das exklusive Oder von einem Wert mit sich selbst immer null ist, kann äquivalent nach $k + 1$ Karten gesucht werden, deren exklusives Oder null ist.
Die Operation XOR von Binärwörtern der Länge $m$ entspricht der Addition von Vektoren aus $\mathbb{F}_2^m$, also dem $m$-dimensionalen Vektorraum über den Körper der Restklassen modulo 2. 
Da es sich bei $\mathbb{F}_2$ um einen Körper handelt, können viele der üblichen Rechnungen, im Besonderen die der linearen Algebra, wie üblich durchgeführt werden. 
Um die gängige Terminologie aus der Kodierungstheorie verwenden zu können, werden die Karten als Spalten einer $m\times n-$Matrix $\mathbf{H}$ dargestellt. Gegenüber den Eingabedatein ist diese Matrix also gespiegelt. 
Das Problem besteht nun daraus, eine Kombination von $t = k + 1$ Spalten aus $\mathbf{H}$ zu finden, die sich (in $\mathbb{F}_2^m$) zu $\mathbf{0}$ addieren. Das entspricht dem Finden von einem $x \in \mathbb{F}^m_2$, für das 
\begin{align*}
    x \mathbf{H} = \mathbf{0} && \text{und} && wt(x) = t
\end{align*}
gilt. $wt(x)$ bezeichnet dabei das Hamming-Gewicht des Vektors, also die Anzahl der von null verschiedenen Stellen. 
Dieses Problem wird als \textit{Minimales-Codewort-Problem} (eng. minimum codeword problem) bezeichnet und wurde bereits eingehend untersucht, unter anderem, da es eine zentrale Rolle für die Sicherheit des McElice-Kryptosystems spielt. 

In diesem Kontext wird $\mathbf{H}$ als Kontrollmatrix (eng. parity check matrix) bezeichnet.
Alle Vektoren $x \in \mathbb{F}^m_2$ für die $x \mathbf{H} = \mathbf{0}$ gilt,  werden als Codewörter bezeichnet und bilden zusammen einen linearen Blockcode. Das Produkt eines beliebigen Vektors $z \in \mathbb{F}^m_2$ mit $\mathbf{H}$ ist ein Syndrom von $z$ bezeichnet. Ein Codewort ist also ein Vektor, dessen Syndrom der Nullvektor ist. 

Berlekamp, McElice und Tilborg \cite{berlekampInherentIntractabilityCertain1978} haben gezeigt, dass dieses Problem zur Klasse der NP-völlständigen Probleme gehört, indem sie das 3-dimensionale Matchingproblem darauf reduziert haben, welches wiederum eins von Karps 21 NP-vollständigen Problemen ist.
Folglich gilt es als sehr unwahrscheinlich, dass es einen Algorithmus gibt, der das Problem in polynomieller Laufzeit löst. 
Im Folgenden sind zwei mögliche Lösungsansätze dargestellt. 
Der zweite ist ein Brute-Force-Ansatz, bei dem das Problem zunächst mithilfe von linearer Algebra vereinfacht und dann durch Ausprobieren aller restlichen Möglichkeiten gelöst wird.
Dieses Verfahren ist sehr effektiv, wenn die die Karten viele Bits haben und und diese Bits linear unabhängig sind, da dann nur sehr wenige Stellen zum Ausprobieren übrig bleiben. Haben die Karten aber wenig Bits oder gibt es sehr viele Karten, kommt der Brute-Force-Ansatz schnell an seine Grenzen.

Der erste Ansatz basiert auf dem \textit{Information-Set-Decoding} und ist in den meisten Fällen sehr viel effizienter, wenn auch immernoch exponentiell in der Laufzeit. Es handelt sich jedoch um einen Las-Vegas-Algorithmus, was heißt, dass die Laufzeit nicht deterministisch ist und der Algorithmus auch nicht verwendet werden kann, um mit Sicherheit zu zeigen, dass es keine Lösung gibt. 
Information-Set-Decoding-Algorithmen sind die besten heute bekannten Algorithmen zum Lösen des Minimalen-Codewort-Problems.

\subsection{Lösungsansatz 1: Information-Set-Decoding}
Da das Problem eine zentrale Rolle für die Sicherheit von mehrern Post-Quantum-Kryptosystemen essentiell ist, wurden verschiedene Algorithmen entwickelt, die in der Praxis deutlich effizienter als ein Brute-Force-Ansatz sind. 
Diese basieren auf dem sogenannten Information-Set-Decoding (ISD). 
Es handelt sich um randomisierte Las-Vegas-Algorithmen, das heißt, dass sie zwar immer das richtige Ergebnis zurückgegeben, die Laufzeit aber variiert. 
Im Kern der Algorithmen steht eine Prozedur, die mit einer Wahrscheinlichkeit $p$ eine Lösung findet. Findet sie keine Lösung, wird sie solang wiederholt, bis sie eine Lösung gefunden hat. 

Die Ursprungsform des ISD geht auf Prange \cite{prangeUseInformationSets1962} zurück.
Später wurde der Algorithmus unter anderem von Lee und Brickell \cite{leeObservationSecurityMcEliece1988} und Stern \cite{sternMethodFindingCodewords1989} verbessert. 
Bei allen Algorithmen gibt es eine Abwägung zwischen der Laufzeit pro Versuch, der Wahrscheinlichkeit, dass der Versucht glückt und der Komplexität des Algorithmus. Der Algorithmus von Lee und Brickell verwendet beispielsweise mehr Zeit pro Versuch als der von Prange, hat dafür aber eine deutlich höhere Erfolgswahrscheinlichkeit. 
Zur Lösung des Problems wurde der Algorithmus von Lee und Brickell verwendet, da er für die gegebenen Beispieldaten völlig ausreichend und nicht unnötig komplex ist. Das vereinfacht auch die theoretische Analyse. 
\begin{algorithm}
    \caption{Finden der Umlegungen}
    \label[]{alg:moves}
    \begin{algorithmic}[1]
        \Procedure{LeeBrickellISD}{$H,t$}
        \While{True}
            \State generiere zufällige Permutation $p$
            \State permutieren die Spalten von $\mathbf{H}$ nach $p$
            \State $\mathbf{H}$ in reduzierte Stufenform (eng. reduced row-echelon form) bringen
            \State sei $I$ die Menge aller Spalten, die nur noch eine Eins enthalten
            \For{ $x \gets $  $p$-große Teilmengen von Spalten $\notin I$}
            \State sei $s$ die Summe der Spalten $x$
            \If{$wt(s) == t - p  + 1$}
                \State $e_m = \begin{cases}
                    1, \text{wenn } m \in p \text{ oder } (s(\textbf{H}_I^{-1}))_m = 1 \\
                    0, \text{andernfalls}
                \end{cases}$ 
                \State permutiere $e$ mit $p^{-1}$
                \State \textbf{return} $e$
            \EndIf
            \EndFor
        \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
Für jeden Versuch wird eine zufällige Permutation generiert und die Spalten von $\mathbf{H}$ entsprechend permutiert. Daraufhin wird $\mathbf{H}$ in reduzierte Stufenform gebracht, beispielsweise mit dem Gauß-Jordan-Algorithmus.
Die Matrix wird nun nach Spalten in zwei Teile geteilt: die Submatrix $\mathbf{H}_I$ enthält alle Spalten, die lediglich eine 1 enthalten und ist folglich invertierbar und eine Submatrix aus den restlichen Spalten. 
Da die Spalten am Anfang zufällig permutiert wurden, wird so eine in jedem Durchlauf zufällige Auswahl von Spalten gefunden, die eine invertierbare Submatrix bilden. 
Nun werden alle Summen von $p$-großen Teilmengen der Spalten $\notin I$ berechent. 
Wenn im gesuchten Wort in dieser zufälligen Permutation genau $p$ Einsen in den Positionen $\notin I$ sind, können die restlichen Einsen in Spalten $\in I$ über lineare Algebra direkt bestimmt werden. 
Das ist genau dann der Fall, wenn das Hamming-Gewicht der Summe genau $t-p$ entspricht, denn zusammen mit den $p$ Einsen aus dem Teil $\notin I$ soll das Wort ein Gewicht von $t$ haben. 
Schließlich wird das Wort rekonstruiert und zurückgegeben. 
$p$ ist ein Parameter des Algorithmus und sollte eine relativ kleine Zahl sein, da die Laufzeit pro Versuch exponentiell mit $p$ steigt. In den meisten Fällen erhöht ein größeres $p$ aber die Erfolgswahrscheinlichkeit pro Versuch, da mehr Einsen in den Stellen $\notin I$ zugelassen werden. Für $p=0$ ergibt sich der ISD-Algorithmus nach Prange.

\subsubsection{Laufzeit- und Speicherkomplexität}
Um die Analyse zu Vereinfachen, wird davon ausgegangen, dass alle Zeilen von $\mathbf{H}$ linear unabhängig sind. Um dies sicherzustellen können vor dem Ausführen des Algorithmuses die linear abhängigen Zeilen mit dem Gauß-Verfahren entfernt und $m$ entsprechend angepasst werden. 
Zunächst wird die Laufzeit- und Speicherkomplexität von einem einzigen Durchlauf analysiert. 
Die zufällige Permutation $p$ kann beispielsweise mit dem Fisher-Yates-Algorithmus in $\Theta(n)$ berechnet werden, das Permutieren der Matrix benötigt dann aber $\Theta(nm)$ Laufzeit, da $n$ Spalten getauscht werden müssen, die jeweils aus $m$ Elementen bestehen.
Daraufhin muss $\mathbf{H}$ in reduzierte Stufenform gebracht werden, beispielsweise mit dem Gauss-Jordan-Algorithmus. 
Eine detaillierte Beschreibung des Gauss-Jordan-Algorithmus und der der dazugehörigen Laufzeitanalyse würde jedoch den Rahmen sprengen.
Die implementierte Version des Algorithmus hat eine Laufzeit von $\Theta(n^2m)$.
Durch diesen Schritt könnte die Anzahl der Zeilen in der Matrix geringer werden. 
Im Worst-Case gehe ich jedoch davon aus, dass alle Zeilen linear unabhängig sind und die Matrix weiterhin $m$ Zeilen hat. 
Die Menge $I$ enthält im Worst-Case folglich genau $m$ Spalten.
Für das Aufteilen in $I$ muss in jeder Zeile nach der ersten Eins gesucht werden, was eine Laufzeit von $O(nm)$ hat.
Die for-Schleife in Zeile 7 benötigt $\binom{n-m}{p}$ Durchläufe.
Das Berechnen der Summe der Spalten, deren Hamming-Gewicht und das Überprüfen der if-Abfrage benötigt $\Theta(pm)$ Laufzeit. 
Ohne die Zeilen 10, 11 und 12 hat die for-Schleife also eine Worst-Case Laufzeit von $\binom{n-m}{p}\Theta(pm)$.
Die Zeilen 10, 11 und 12 werden pro Aufruf der Prozedur nur ein einziges Mal ausgeführt. Werden die in Zeile 6 gefundenen Positionen gespeichert und wiederverwendet, ist die Berechnung von $e$ in $\Theta(mp)$ möglich. 
Insgesamt ergibt sich pro Iteration eine Laufzeit von 
\begin{align*}
    T(n, m) &= \Theta(n) + \Theta(nm) + \Theta(n^2m) + \binom{n-m}{p} \cdot \Theta(pm) + \Theta(mp)\\
            &= \Theta(n^2m) + \binom{n-m}{p}\cdot \Theta(pm)
\end{align*}
% Mit $\binom{n}{k} = O\left(\left(\frac{en}{k}\right)^k\right)$ ergibt sich
% \begin{align*}
%     T(n, m) &= O\left(nm \cdot \max(n, m) +\left(\frac{n-m}{p}\right)^p \cdot pm\right)
% \end{align*}
Es zeigt sich also, das die Laufzeit für eine einzige Iteration exponentiell mit $p$ und polynomiell mit $n$ und $m$ steigt. Wie bereits erwähnt sollte also für den Parameter $p$ eine relativ kleine Zahl gewählt werden.
Für die erwartete Gesamtlaufzeit ist aber nicht nur die Laufzeit einer Iteration wichtig, sondern auch die Erfolgswahscheinlichkeit.

Die $t$ Einsen könnten in $n$ verschiedenen Positionen auftreten, sodass es $\binom{n}{t}$ Möglichkeiten für $e$ gibt. Der Algorithmus ist immer dann erfolgreich, wenn genau $p$ der Einsen in $n-m$ Spalten und $t-p$ der Einsen in den $m$ Spalten $\in I$ sind. Durch die zufällige Permutation sind die Einsen gleichmäßg zufällig auf die Spalten verteilt. Folglich gilt nach dem Lottomodell für die Erfolgswahscheinlichkeit eines einzigen Versuchs
\begin{align*}
    P(\text{Erfolg}) = \frac{\binom{n-m}{p} \cdot \binom{m}{t-p}}{\binom{n}{t}}.
\end{align*}

Der Erwartungwert für die Anzahl der Versuche bis ein Ereignis $X$ der Wahrscheinlichkeit $P(X)$ auftritt ist $E(x) = \frac{1}{P(X)}$.
Folglich gilt, wenn $X$ die Anzahl der Durchläufe ist,
\begin{align*}
    E(X) = \frac{\binom{n}{t}}{\binom{n-m}{p} \cdot \binom{m}{t-p}}.
\end{align*} 
In der Praxis ist dieser Wert gerade für Beispiele, in denen $t$ im Vergleich zu $n-m$ relativ klein ist, ebenfalls vergleichsweise gering. 
Im Folgenden ist $E(X)$ für verschiedene $p$ für die Beispiele von der BWINF-Website aufgeführt.
\begin{table}[H]
    \centering
    \begin{tabular}{lrrr}
        \toprule
    
        Beispiel & $E(X)$, $p=0$ & $E(X)$, $p=1$ & $E(X)$, $p=2$ \\\midrule
        stapel1.txt &1.33	&4.00 &  \\
        stapel1.txt &1.82&	2.22&  \\
        stapel2.txt &1.11	&10.09&  \\
    stapel3.txt &13.68&	4.45&	3.31 \\
    stapel4.txt	&51.58&	10.44	&4.78 \\
    stapel5.txt	&332.57	&29.34&	6.63\\\bottomrule
    \end{tabular}
\end{table}
Es ist zu erkennen, dass meistens schon wenige Durchläufe genügen, um eine Lösung zu finden. 
Für die Implementation habe ich $p=1$ verwendet, da die Erfolgswahrscheinlichkeit auch für größere Instanzen ausreicht und die Implementation vereinfacht wird. Es wäre natürlich auch denkbar, $p$ an die Dimensionen der Eingabedaten und die Geschwindigkeit des Computers dynamisch anzupassen.

Der Erwartungswert für die Laufzeit liegt damit bei 
\begin{align*}
    E(T) &= \frac{\binom{n}{t}}{\binom{n-m}{p} \cdot \binom{m}{t-p}} \cdot \left(\Theta(n^2m) + \binom{n-m}{p}\cdot \Theta(pm)\right) \\
\end{align*}

Die Speicherkomplexität beträgt $\Theta(nm)$, da die $m \times n$-Matrix einmal kopiert werden muss, um die Permutation anzuwenden. Theoretisch wäre diese Kopie durch ein effizienteres Verfahren zur Anwendung der Permutation vermeidbar, der Speicherbedarf wird aber vermutlich keine Einschränkung für den Algorithmus darstellen, da die Laufzeiten bei derartig großen Instanzen nicht mehr vertretbar wäre. 
Neben der Kopie der Matrix werden die Permutation und die Positionen der Pivots des Gauss-Algorithmus gepeichert, was zusätzlich $O(n)$ Speichereinheiten benötigt. 

\subsection{Lösungsansatz 2: Brute-Force}
Die Idee des Brute-Force-Ansatzes ist es, die Matrix zunächst über das Gauß-Verfahrens in Stufenform zu bringen. Daraufhin muss resubstituiert werden, um zur reduzierten Stufenform zu gelangen. 
Da die Gleichungssysteme aber unterbestimmt sind, gibt es freie Variablen.
In diesem Fall wird rekursiv versucht, zuerst die 0 und dann die 1 an dieser Stelle zu substituieren. 
Hat das so gefundene Wort die richtige Anzahl an Einsen, wird es als Lösung ausgegeben, andernfalls werden per Backtracking andere Werte für die freien Variablen eingesetzt. 
Das Verfahren kann unter anderem dadurch etwas beschleunigt werden, dass abgebrochen wird, wenn bereits die maximale Anzahl an Einsen verwendet wurde oder es nicht mehr genug Variablen gibt, um die nötige Anzahl an Einsen zu erreichen. 

Es folgt eine grobe Abschätzung der Laufzeit dieses Verfahrens.
Die Anzahl der freien Variable, für die also per Backtracking potentiell beide Möglichkeiten ausprobiert werden müssen, ist $n-m$. Pro rekursivem Aufruf wird im Worst-Case $O(nm)$ Zeit für die Substitution verwendet. Zusammen mit dem Gauß-Jordan-Algorithmus, der zuvor ausgeführt wird, ergibt sich eine Laufzeit von 
\begin{align*}
    T(n,m) = \Theta(n^2m) + O(2^{n-m})
\end{align*}.
Das ist bei Instanzen, bei denen $n$ und $m$ relativ nah beineinander liegen bereits deutlich besser als ein reines Brute-Force, bei dem alle Kombinationen von Karten ausprobiert werden würden, was eine Laufzeit von $O(2^n)$ benötigen würde.
Die Laufzeit ist dennoch in der Praxis meist deutlich langsamer als die des Information-Set-Decoding.  
Das Verfahren hat aber den Vorteil, dass es vollständig derterministisch ist und es auch mit Sicherheit bestätigen kann, dass es keine Lösung gibt, wenn das der Fall ist. 

\subsection{Erweiterung: Generierung von größeren Beispielen}
Beide Ansätze können die Beispiele vom BWINF in angemessener Zeit lösen.
Der Information-Set-Decoding-Algorithmus ist aber in der Lage, auch deutlich größere Beispiele zu Lösen. Daher wurde die Aufgabe so erweitert, dass auch beispiele von 2-4-facher Größe gelöst werden sollen. Mit einem reinen Brute-Force-Ansatz sind derart große Beispiele dann nich mehr lösbar. Zusätzlich müssen größere Beispieldatein generiert werden.
Hierfür wäre es denkbar, ähnlich wie im McElice-Kryptosystem einen linearen Code mit bestimmten minmalen Distanz zu generieren. So könnte man sich sicher sein, dass es nur die eine Menge an Karten gibt, die im exklusiven Oder null ergeben. Allerdings können auch einfach Zufällig generierte Karten verwendet werden, da es bei zufällig ausgewählten Vektoren äußerst unwahrscheinlich ist, dass es eine zweite Lösung gibt. Danach werden $k$ Karten ausgewählt und eine weitere Karte durch das exklusive Oder der $k$ Karten ausgetauscht.
\subsection{Teilaufgabe b)}
Nachdem Zara die $k+1$ richtigen Karten gefunden hat, möchte sie Haus $h$ aufschließen. 
\section{Umsetzung}
Die Lösung wurde in \texttt{C++} implementiert. Die beiden Ansätze wurden in jeweils einer Klasse, \lstinline{ISDSolver} und \lstinline{BruteforceSolver} implementiert. Eine \lstinline{Utils}-Klasse enthält Methoden, die von beiden Algorithmen verwendet werden, beispielsweise eine Implementation des Gauß-Jordan-Algorithmus über $\mathbb{F}_2$.

Zunächst wird die Implementation des Information-Set-Decoding Algorithmus von Lee und Brickell erläutert.
Vor dem eigentlichen Algorithmus wird, wie bereits kurz erwähnt, einmal der Gauß-Jordan-Algorithmus auf der Matrix ausgeführt. 
Das führt dazu, dass linear abhängige Zeilen entfernt werden und die Implementation vereinfacht wird, da die Matrix nun immer mindestens so viele Spalten wie Zeilen hat.
Bei jedem Versuch wird eine zufällige Permutation für die Spalten über die \lstinline{std::shuffle} Methode der \texttt{C++}-Standardbibliothek generiert und die Matrix nach dieser Permutation kopiert. 
Danach wird der Gauß-Jordan-Algorithmus ausgeführt, die Pivots gesucht und deren Zeilen und Spalten gespeichert. 
Hier wird der Gauß-Jordan-Algorithmus sehr häufig, nähmlich in jedem Versuch, ausgeführt und ist der Flaschenhals für die Laufzeit. 
Daher wird eine effizientere Version des Algorithmus verwendet, die statt Vektoren aus der Standardbibliothek \lstinline{dynamic_bitset}s aus der Boost-Bibliothek verwendet.
Außerdem wird das Entfernen von null-Zeilen ausgelassen, da bereits zu Beginn die linear abhängigen Zeilen entfernt wurden und statt \lstinline{std::swap} wird der XOR-Swap-Algorithmus verwendet, da die Speicherverwaltung für die temporäre Variable in \lstinline{std::swap} viel Zeit benötigt.

Da $p=1$ verwendet wird, kann nun einfach über alle Spalten iteriert werden, in denen keine Pivots gefunden wurde und deren Hamming-Gewicht berechnet werden. 
Hat die Spalte das richtige Gewicht, können über die gespeicherten Pivot-Positionen die notwendigen Bits gefunden werden, ohne wirklich das Syndrom mit dem Inversen der Submatrix $\mathbf{H}_I$ zu multiplizieren.
Schlussendlich wird die Permutation rückgängig gemacht und das Ergebnis ausgegeben. 
Aufgrund seiner Struktur ist dieser Algorithmus sehr einfach zu parallelisieren: alle Kerne eines Prozessors können den Algorithmus unabhängig voneinader ausführen, bis ein Thread eine Lösung gefunden hat.
Um dieses Potential auch auszuschöpfen wurde OpenMP verwendet, um die Hauptschleife mehrfach gleichzeitig auszuführen.

Der Brute-Force-Algorithmus ist deutlich ineffizienter und musste stark optimiert werden, um alle Beispiele von der BwInf-Website in anehmbarer Zeit lösen zu können. 
Dafür werden in der \lstinline{vorberechnen()} Funktion einige Dinge vorberechnet, sodass beim eigentlichen berechnen weniger in der Matrix gesucht werden muss. 
Konkret wird vorberechnet, welche Spalten Pivotelemente haben, da die Werte für diese Spalten durch Resubstitution ermittelt werden können, statt 0 und 1 auszuprobieren. 
Für die Berechnung ist außerdem wichtig, in welcher Spalte das jeweilige Pivotelement ist. 
Daraufhin werden alle Indizes von Einsen in den Zeilen und Spalten vorberechnet, um effizient über diese Positionen iterieren zu können. 
Zuletzt werden alle Zeilen und Spalten in \lstinline{std::bitset<>}s gespeichert, ebenfalls um effizienter mit ihnen Rechnen zu können.
Da \lstinline{std::bitset}s eine zur Compilezeit konstante Anzahl an Bits benötigen, braucht auch die \lstinline{BruteforceSolver}-Klasse einen zur Compilezeit konstanten Template-Parameter, der die Anzahl der Bits, mit der gerechnet werden soll, angibt.  
Zur Laufzeit wird dann je nach Größe der Eingabedatei zwischen 256, 1024 und 8129 Bits ausgewählt. 
Die \lstinline{dynamic_bitset}s aus der Boost-Bibliothek benötigen viel Laufzeit zur dynamischen Zuweisung ihres Speicherplatzes, weshalb es in diesem Fall effizienter ist, verschiedene Größen von statischen \lstinline{std::bitset}s zu verwenden.
Das eigentliche Lösen wurde dann Rekursiv in der Methode \lstinline{resub} implementiert.
Die Methode bekommt eine Spalte, ab der substituiert werden soll, das bisherige Codewort, die Anzahl der Einsen, die noch gesetzt werden können und das Syndrom, also die Summe der Spalten, die im aktuellen Codewort eine Eins haben. 
Die letzteren beiden Argumente könnten auch bei jedem Methodenaufruf neu berechent werden, werden aber aus Performancegründen übergeben. 
Daraufhin wird solange resubstituiert, bis eine Spalte erreicht wird, die kein Pivot-Element enthält, wo also sowohl 0 als auch 1 möglich wären.
Die Suche kann optimiert werden, indem abgebrochen wird, wenn es nicht mehr genug Spalten gibt, um das Zielgewicht zu erreichen oder das Zielgewicht bereits überschritten wurde. 
Schlussendlich wird nacheinander 0 und 1 für die freie Variable eingesetzt und die Methode rekursiv für die nächste Variable aufgerufen. 

Da der Brute-Force-Ansatz mitunter sehr lange brauchen kann, ist es wichtig, dass der Benutzer Feedback über den Fortschritt bekommt. 
Es gestaltet sich aber als durchaus schwierig, den aktuellen Fortschritt zu berechen. 
Daher werden die effekte der beiden Abbruchbedingungen vernachlässigt. 
Dann kann aus dem aktuellen Codewort der Fortschritt ermittelt werden.
In der Praxis ist dieser Fortschritt am Anfang des Algorithmus zu pessimistisch, reicht aber aus, um den Nutzer eine grobe Idee zu geben, wie lange der Algorithmus noch brauchen wird. 
Da der Algorithmus bricht ab, wenn er eine Lösung gefunden hat, der Fortschritt bezieht sich jedoch auf die Zeit, die benötigt wird, um alle Möglichkeiten auszuprobieren. 
Das heißt, dass der Algorithmus meist schneller terminiert, als man es nach dem Fortschritt erwarten würde -- dies zu verhindern würde aber keinen Sinn ergeben, da der Algorithmus natürlich noch nicht weiß, wann er eine Lösung finden wird. 
Der Code zum Ausgeben des Fortschrittes ist in \lstinline{printProgress()} zu finden.

Zum Compilieren des Codes wird ein \texttt{C++-14}-fähiger Compiler benötigt, der zusätzlich OpenMP unterstützt.
Als Build-System wird CMake verwendet. 
Der Code kann wie üblich über 
\begin{lstlisting}[language=bash]
    mkdir build
    cd build
    cmake ..
    cmake --build . 
\end{lstlisting}    
kompiliert und dann mit \lstinline[language=bash]{./stapel [algorithmus] [Pfad]} ausgeführt werden, wobei zwischen den Algorithmen \lstinline{isd} für Information-Set-Decding und \lstinline{bruteforce} für den Bruteforce-Ansatz ausgewählt werden kann. Der Pfad sollte auf die zu lösende Eingabedatei zeigen. Der Code wurde unter Ubuntu 20.04. getestet.
\section{Beispiele}
Genügend Beispiele einbinden! Die Beispiele von der BwInf-Webseite sollten hier diskutiert werden, aber auch eigene Beispiele sind sehr gut – besonders wenn sie Spezialfälle abdecken. Aber bitte nicht 30 Seiten Programmausgabe hier einfügen!

\section{Quellcode}
Unwichtige Teile des Programms sollen hier nicht abgedruckt werden. Dieser Teil sollte nicht mehr als 2–3 Seiten umfassen, maximal 10.

\section{Quellen}
\printbibliography

\end{document}
